var hostname = "https://scott.cranton.com";
var index = lunr(function () {
    this.field('title')
    this.field('content', {boost: 10})
    this.field('category')
    this.field('tags')
    this.ref('id')
});



    index.add({
      title: "Kubernetes Ingress Past, Present, and Future",
      category: null,
      content: "\n    \n    Photo by Luke Porter.\n\n\nOverview\n\nThis post was inspired by listening to the February 19, 2019, Kubernetes Podcast,\n“Ingress, with Tim Hockin.” The Kubernetes Podcast is turning out\nto be a very well done podcast overall, and well worth the listen. In the Ingress episode, the podcast interviews\nTim Hockin who’s one of the original Kubernetes co-founders, a team lead on the Kubernetes predecessor Borg/Omega,\nand is still very active within the Kubernetes community such as chairing the Kubernetes Network Special Interest Group\nthat currently own the Ingress specification. Tim talks in the podcast about the history of the\nKubernetes Ingress, current developments around\nIngress, and proposed futures. This talk inspired me to reflect on both Ingress Controllers (realizes the implementation of\nIngress manifest) and Ingress the concept (allow client outside the Kubernetes cluster to access services running in\nthe Kubernetes cluster).\n\nSo what’s a Kubernetes Ingress?\n\nTo paraphrase from the Kubernetes Ingress\ndocumentation, Ingress is an L7 network service that exposes HTTP(S) routes from outside to inside a Kubernetes cluster.\nA Kubernetes cluster may have one or more Ingress Controllers\nrunning, and each controller manages service reachability, load balancing, TLS/SSL termination, and other services for\nthat controller’s associated routes.\n\n\n\nEach Ingress manifest includes an annotation that specifies that indicates which\nIngress controller should manage that Ingress instance. For example, to have Solo.io Gloo\nmanage an Ingress, you could specify the following. Note the annotation kubernetes.io/ingress.class: gloo.\n\napiVersion: extensions/v1beta1\nkind: Ingress\nmetadata:\n  annotations:\n    kubernetes.io/ingress.class: gloo\n  labels:\n    chart: jsonplaceholder-v0.1.0\n  name: jsonplaceholder-jsonplaceholder\n  namespace: default\nspec:\n  rules:\n  - host: gloo.example.com\n    http:\n      paths:\n      - path: /.*\n        backend:\n          serviceName: jsonplaceholder-jsonplaceholder\n          servicePort: 8080\n\n\nIngress Challenges\n\nIngress has existed as a beta extension since Kubernetes 1.1 as it’s proven to be a lowest common denominator API. The\nNGNIX community Ingress Controller is used by many in production, but that controller requires the use of one of many\nNGNIX specific Ingress Annotations\nfor all but the simplest use cases. The current API has many limitations like all referenced services and secrets MUST\nbe in the same namespace as the Ingress, i.e., no cross namespace referencing. And there have been long debates about\nhow exactly to interpret the path attribute; is it a regular expression like the documentation implies OR is it a path\nprefix like some controllers like NGNIX implement. These challenges have made it, in practice, difficult to have\nan Ingress manifest that is portable across implementations. The current Ingress manifest has also proven difficult to\nround trip sync with Custom Resources (CRD)\nas CRDs have proven to be a beneficial way to extend Kubernetes.\n\nWhat’s Next for Ingress?\n\nIn the podcast, Tim Hockin says given how many are using the current beta Ingress spec in production, there is a push\nto move the existing Ingress spec to 1.0 GA status, and start work on a next-generation spec, either an Ingress v2 or\nbreaking up Ingress across multiple CRDs. Tim mentions how the Kubernetes community is looking at several\nEnvoy based Ingress implementations for inspiration for the next generation of Ingress. For\nexample, Heptio Contour has created a very interesting, and implementation\nneutral CRDs called Ingress Route. An Ingress Route\nlooks to address the governance challenges with Ingress, for example, if a company wants to expose a\n/eng route path there are many challenges with the current Ingress model as you can have conflicting Ingress\nmanifests for the route /eng. Ingress Route provides a way to create governance and delegation such as cluster admins\ncan define a virtual host /eng and delegate implementation explicitly to the eng namespace, and this prevents others\nfrom overriding that route.\n\nThe Istio community, also based on Envoy like Heptio Contour, are also defining Ingress CRDs.\n\nIt will be fascinating to see how Ingress evolves in the not too distant future.\n\nDemo Time\n\nI find it helpful to see some code to make concepts more concrete, so let’s run through a few examples of Ingress and\nbeyond.\n\nFor this example, I’m going to use a Kubernetes service created from https://jsonplaceholder.typicode.com/, which\nprovides a quick set of REST APIs that provide different JSON output that can be helpful for testing. It’s based on\na Node.js json-server - it’s very cool and worth looking at independently. I\nforked the original GitHub jsonplaceholder repository, ran Draft create\non the project, and made a couple of tweaks to the generated Helm chart. Draft is a super fast and easy way\nto bootstrap existing code into Kubernetes. I’m running all of this locally using minikube.\n\nThe jsonplaceholder service comes with six common resources each of which returns several JSON objects. For this\nexample, we’ll be getting the first user resource at /users/1.\n\n\n  /posts\t100 posts\n  /comments\t500 comments\n  /albums\t100 albums\n  /photos\t5000 photos\n  /todos\t200 todos\n  /users\t10 users\n\n\nFollowing is the script to try this example yourself, and there’s also an asciinema playback so you can see\nwhat it looks like running on my machine (playback is sped up). We’ll unpack what’s happening following the playback.\n\n# Install tooling\nbrew update\nbrew cask install minikube\nbrew install kubernetes-cli \\\n  kubernetes-helm \\\n  azure/draft/draft \\\n  glooctl\n\n# Create and set up local Kubernetes Cluster\nminikube start\nhelm init\ndraft init\nglooctl install ingress\n\n# Draft runs better locally if you configure\n# against minikube docker daemon\neval $(minikube docker-env)\n\n# Get and run the example\ngit clone https://github.com/scranton/jsonplaceholder.git\ncd jsonplaceholder\ndraft up\n\n# Validate all is running\nkubectl get all --namespace default\nkubectl get all --namespace gloo-system\nkubectl get ingress --namespace default\ncurl --header \"Host: gloo.example.com\" \\\n  $(glooctl proxy url --name ingress-proxy)/users/1\n\n\n\n\nWhat Happened?\n\nWe installed local tooling (you can check respective websites for full install details)\n\n\n  minikube\n  kubectl\n  Helm\n  Draft\n  Gloo\n\n\nWe then started up a local Kubernetes cluster (minikube) and initialized Helm and Draft. We also installed\nGloo ingress into our local cluster.\n\nWe then git clone our example up and used draft up to build and deploy it to our cluster. Let’s spend a minute on\nwhat happened in this step. I originally forked the jsonplaceholder GitHub repository and ran draft create against\nits code. Draft autodetects the source code language, in this case, Node.js, and create both a Dockerfile that builds\nour example application into an image container and creates a default Helm chart. I then made a few minor tweaks to the\nHelm chart to enable its Ingress. Let’s look at that Ingress manifest. The main changes are the addition of the\ningress.class: gloo annotation to mark this Ingress for Gloo’s Ingress Controller. And the host is set to\ngloo.example.com, which is why our curl statement set curl --header \"Host: gloo.example.com\".\n\n\n\n{{- if .Values.ingress.enabled -}}\napiVersion: extensions/v1beta1\nkind: Ingress\nmetadata:\n  name: {{ template \"fullname\" . }}\n  labels:\n    chart: \"{{ .Chart.Name }}-{{ .Chart.Version | replace \"+\" \"_\" }}\"\n  annotations:\n    kubernetes.io/ingress.class: {{ .Values.ingress.class }}\nspec:\n  rules:\n  - host: {{ .Values.ingress.basedomain }}\n    http:\n      paths:\n      - path: /.*\n        backend:\n          serviceName: {{ template \"fullname\" . }}\n          servicePort: {{ .Values.service.externalPort }}\n{{- end -}}\n\n  charts/template/ingress.yaml\n\n\nFor more examples of using Gloo as an basic Ingress controller you can check out\nKubernetes Ingress Control using Gloo.\n\nYou may have also noticed the call to $(glooctl proxy url --name ingress-proxy) in the curl command. This is needed\nwhen you’re running in a local environment like minikube, and you need to get the host IP and port of the Gloo proxy\nserver. When deployed to a Cloud Provider like Google or AWS, then those environments would associate a static IP and\nallow port 80 (or port 443 for HTTPS) to be used, and that static IP would be registered with a DNS server, i.e., \nwhen Gloo is deployed to a cloud-managed Kubernetes you could do curl http://gloo.example.com/users/1.\n\nIngress Example Challenges\n\nLet’s say we wanted to remap the exiting /users/1 to /people/1 as users are people too. That becomes tricky with\nIngress manifests as we can set up a second rule for /people, but we need to rewrite that path to /users before\nsending to our service as it doesn’t know how to handle requests for /people. If you were using the NGNIX ingress, you\ncould add another annotation nginx.ingress.kubernetes.io/rewrite-target: /, but now we’re adding implementation\nspecific annotations, that is the nginx annotation won’t be recognized by other Ingress Controllers. And annotations\nare a flat name space so adding lots of annotations can get quite messy, which is part of why\nCustom Resources (CRD) was\ncreated. Let’s see what the original route, and our path re-writing route, would look like in a CRD based Ingress: Gloo.\n\nGloo Virtual Services\n\nGloo uses a concept called Virtual Service that is\nderived from similar ideas in Istio/Envoy and is conceptually equivalent to an Ingress resource. Easiest to show\nyou the equivalent of example Ingress we’ve created so far in a Gloo Virtual Service.\n\napiVersion: gateway.solo.io/v1\nkind: VirtualService\nmetadata:\n  name: default\n  namespace: gloo-system\nspec:\n  virtualHost:\n    domains:\n    - gloo.example.com\n    routes:\n    - matcher:\n        prefix: /\n      routeAction:\n        single:\n          upstream:\n            name: default-jsonplaceholder-jsonplaceholder-8080\n            namespace: gloo-system\n\n\nYou’ll notice that it looks very similar to the Ingress we had previously created with a few subtle changes. The path\nspecifier is prefix: / which is generally what people intend, i.e., if the beginning of the request message path\nmatches the route path specifier than apply the route actions. If we wanted to exactly match the previous Ingress, we could use\nregex: /.* instead. Virtual Services allow you to specify paths by: prefix, exact, and regular expression. You can\nalso see that instead of backend: with serviceName and servicePort, a Virtual Service has a routeAction that\ndelegates to a single upstream. Gloo upstreams are auto-discovered and can refer to Kubernetes Services AND\nREST/gRPC function, cloud functions like AWS Lambda and Google Functions, and other external to Kubernetes services.\n\nMore details on Gloo at:\n\n\n  Routing with Gloo Function Gateway\n  5 minutes with Gloo - The Anatomy of a VirtualService\n\n\nLet’s go back to our example, and update our Virtual Service to do the path rewrite we wanted, i.e., /people =&gt; /users\n\napiVersion: gateway.solo.io/v1\nkind: VirtualService\nmetadata:\n  name: default\n  namespace: gloo-system\nspec:\n  virtualHost:\n    domains:\n    - gloo.example.com\n    routes:\n    - matcher:\n        prefix: /people\n      routeAction:\n        single:\n          upstream:\n            name: default-jsonplaceholder-jsonplaceholder-8080\n            namespace: gloo-system\n      routePlugins:\n        prefixRewrite:\n          prefixRewrite: /users\n    - matcher:\n        prefix: /\n      routeAction:\n        single:\n          upstream:\n            name: default-jsonplaceholder-jsonplaceholder-8080\n            namespace: gloo-system\n\n\nWe’ve added a second route matcher, just like adding a second route path in an Ingress, and specified prefix: /people.\nThis will match all requests that start with /people, and all other calls to the gloo.example.com domain will be\nhandled by the other route matcher. We also added a routePlugins section that will rewrite the request path to /users such\nthat our service will now correctly handle our request. Route Plugins\nallow you to perform many operations on both the request to the upstream service and the response back from the upstream\nservice. Best shown with an example, so for our new /people route let’s also transform the response to both add\na new header x-test-phone with a value from the response body, and let’s transform the response body to return a\ncouple of fields: name, and the address/street and address/city.\n\npiVersion: gateway.solo.io/v1\nkind: VirtualService\nmetadata:\n  creationTimestamp: \"2019-04-08T21:43:45Z\"\n  generation: 1\n  name: default\n  namespace: gloo-system\n  resourceVersion: \"772\"\n  selfLink: /apis/gateway.solo.io/v1/namespaces/gloo-system/virtualservices/default\n  uid: 6267ee31-5a47-11e9-bc30-867df7be8a8a\nspec:\n  virtualHost:\n    domains:\n    - gloo.example.com\n    routes:\n    - matcher:\n        prefix: /people\n      routeAction:\n        single:\n          upstream:\n            name: default-jsonplaceholder-jsonplaceholder-8080\n            namespace: gloo-system\n      routePlugins:\n        prefixRewrite:\n          prefixRewrite: /users\n        transformations:\n          responseTransformation:\n            transformation_template:\n              body:\n                text: '{ \"name\": \"{{ name }}\", \"address\":\n                  { \"street\": \"{{ address.street }}\",\n                    \"city\": \"{{ address.city }}\" } }'\n              headers:\n                x-test-phone:\n                  text: '{{ phone }}'\n    - matcher:\n        prefix: /\n      routeAction:\n        single:\n          upstream:\n            name: default-jsonplaceholder-jsonplaceholder-8080\n            namespace: gloo-system\n\nLet’s see what that looks like. My example GitHub repository already included\nthe full Gloo Virtual Service we just examined. We need to configure Gloo for\ngateway which means adding another proxy to handle Virtual Services in\naddition to Ingress resources. We’ll use draft up to ensure our example is\nfully deployed including the full Virtual Service, and then we’ll call both\n/users/1 and /people/1 to see the differences.\n\n# Install Gloo and update example\nglooctl install gateway\ndraft up\n\n# Call service\ncurl --verbose --header \"Host: gloo.example.com\" \\\n  $(glooctl proxy url --name gateway-proxy)/users/1\n\ncurl --verbose --header \"Host: gloo.example.com\" \\\n  $(glooctl proxy url --name gateway-proxy)/people/1\n\n\n\n\n\n\nOk, well not that mind-blowing if you’ve used other L7 networking products or done other integration work, but still\npretty cool relative to standard Ingress objects. Gloo is using Inja Templates to process the JSON response body.\nMore details in the Gloo documentation.\n\nSummary\n\nIn this article, we touched on some of the history and difficulties with the existing Kubernetes Ingress resources. Ingress\nresources continue to play a role within Kubernetes deployments despite the many challenges that annotation-based\nextensions have. Kubernetes Custom Resources (CRDs) was created to address some of those extension challenges and\ncan provide a cleaner way to extend Kubernetes as you saw in the Gloo Ingress and Gateway examples. I’m a big believer\nin the potential of Envoy based solutions as are others in the Istio and Contour communities, and it will be exciting\nto see how the Kubernetes community decides to evolve Ingress after they finally move the existing\nresource spec to GA status.\n",
      tags: ["Gloo","Ingress","Kubernetes"],
      id: 0
    });
    

    index.add({
      title: "Automating your Services with Knative and Solo.io Gloo",
      category: null,
      content: "Knative is talked about a great deal, especially around how its capabilities can\nhelp provide more standard building blocks on top of Kubernetes for building microservices and serverless like services,\ne.g., scale to zero, and scale on demand. Knative high level has three capability areas: building, serving, and eventing.\nThis post will provide some examples around Knative Build and Knative Serving with Solo.io Gloo.\n\nKnative Serving initially included all of Istio only to use a small fraction of its capabilities around being a\nKubernetes cluster ingress. Recently the Knative team added Solo.io Gloo as\nan alternative to Istio. More details are available in Gloo, Knative and the future of Serverless\nand Gloo, by Solo.io, is the first alternative to Istio on Knative.\n\nThis post shows a quick example of Knative Building, Knative Serving, and Gloo integration.\n\nAll of the Kubernetes Manifests are located in the following GitHub repository\nhttps://github.com/scranton/helloworld-knative. I encourage you to fork that repository to help you try these examples\nyourself.\n\nSetup\n\nThese instructions assume you are running on a clean, recent minikube\ninstall locally, and that you also have kubectl available locally.\n\nInstall Gloo\n\nOn Mac or Linux, the quickest option is to use Homebrew. Full Gloo\ninstall instructions at Gloo documentation.\n\nbrew install glooctl\n\n\nThen assuming you’ve got a running minikube, and kubectl set up against that minikube instance, i.e., kubectl\nconfig current-context returns minikube, run the following to install Gloo with Knative Serving.\n\nglooctl install knative\n\n\nDeploy existing example image\n\nI’ve already built this example, and have hosted the image publicly in my Docker Hub repository.\nTo use Knative to serve up this existing image, you need to do the following command.\n\nkubectl apply --filename service.yaml\n\n\nVerify the domain URL for the service. It should be helloworld-go.default.example.com.\n\nkubectl get kservice helloworld-go \\\n  --namespace default \\\n  --output=custom-columns=NAME:.metadata.name,DOMAIN:.status.domain\n\n\nAnd call the service. Note: the curl --connect-to option is only required when calling locally against minikube as\nthat option will add the correct host and sni headers to the request, and send the request to the host and port pair\nreturned from glooctl proxy address.\n\ncurl --connect-to helloworld-go.default.example.com:80:$(glooctl proxy address --name clusteringress-proxy) http://helloworld-go.default.example.com\n\n\nTo clean up, delete the resources.\n\nkubectl delete --filename service.yaml\n\n\nBuild locally, and deploy using Knative Serving\n\nRun docker build with your Docker Hub username.\n\ndocker build -t ${DOCKER_USERNAME}/helloworld-go .\ndocker push ${DOCKER_USERNAME}/helloworld-go\n\n\nDeploy the service. Again, make sure you updated username in service.yaml file, i.e., replace image reference\ndocker.io/scottcranton/helloworld-go with your Docker Hub username.\n\nkubectl apply --filename service.yaml\n\n\nVerify domain URL for service. Should be helloworld-go.default.example.com.\n\nkubectl get kservice helloworld-go \\\n  --namespace default \\\n  --output=custom-columns=NAME:.metadata.name,DOMAIN:.status.domain\n\n\nAnd test your service.\n\ncurl --connect-to helloworld-go.default.example.com:80:$(glooctl proxy address --name clusteringress-proxy) http://helloworld-go.default.example.com\n\n\nTo clean up, delete the resources.\n\nkubectl delete --filename service.yaml\n\n\nBuild using Knative Build, and deploy using Knative Serving\n\nTo install Knative Build, do the following. I’m using the kaniko build template, so you’ll also need to install that\nas well.\n\nkubectl apply \\\n  --filename https://github.com/knative/build/releases/download/v0.4.0/build.yaml\n\nkubectl apply \\\n  --filename https://raw.githubusercontent.com/knative/build-templates/master/kaniko/kaniko.yaml\n\n\nTo verify the Knative Build install, do the following.\n\nkubectl get pods --namespace knative-build\n\n\nI’d encourage forking my example GitHub repository https://github.com/scranton/helloworld-knative, so you can push\ncode changes and see them in your environment.\n\nCreate a Kubernetes secret for your Docker Hub account that will allow Knative build to push your image. You also need to annotate the secret to indicate it’s for Docker. More details in Guiding credential selection.\n\nkubectl create secret generic basic-user-pass \\\n  --type=\"kubernetes.io/basic-auth\" \\\n  --from-literal=username=${DOCKER_USERNAME} \\\n  --from-literal=password=${DOCKER_PASSWORD}\n\nkubectl annotate secret basic-user-pass \\\n  build.knative.dev/docker-0=https://index.docker.io/v1/\n\n\nIt should result in a secret like the following.\n\nkubectl describe secret basic-user-pass\n\nName:         basic-user-pass\nNamespace:    default\nLabels:       &lt;none&gt;\nAnnotations:  build.knative.dev/docker-0: https://index.docker.io/v1/\n\nType:  kubernetes.io/basic-auth\n\nData\n====\nusername:  12 bytes\npassword:  24 bytes\n\n\nVerify that serviceaccount.yaml references your secret.\n\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: build-bot\nsecrets:\n  - name: basic-user-pass\n\n\nUpdate service-build.yaml with your GitHub and Docker usernames. This manifest will use Knative Build to create an\nimage using the kaniko-build build template and deploy the service using Knative Serving with Gloo.\n\napiVersion: serving.knative.dev/v1alpha1\nkind: Service\nmetadata:\n  name: helloworld-go\n  namespace: default\nspec:\n  runLatest:\n    configuration:\n      build:\n        apiVersion: build.knative.dev/v1alpha1\n        kind: Build\n        metadata:\n          name: kaniko-build\n        spec:\n          serviceAccountName: build-bot\n          source:\n            git:\n              url: https://github.com/{ GitHub username }/helloworld-knative\n              revision: master\n          template:\n            name: kaniko\n            arguments:\n              - name: IMAGE\n                value: docker.io/{ Docker Hub username }/helloworld-go\n          timeout: 10m\n      revisionTemplate:\n        spec:\n          container:\n            image: docker.io/{ Docker Hub username }/helloworld-go\n            imagePullPolicy: Always\n            env:\n              - name: TARGET\n                value: \"Go Sample v1\"\n\n\nTo deploy, apply the manifests.\n\nkubectl apply \\\n  --filename serviceaccount.yaml \\\n  --filename service-build.yaml\n\n\nThen you can watch the build and deployment happening.\n\nkubectl get pods --watch\n\n\nOnce you see all the helloworld-go-0000x-deployment-.... pods are ready, then you can Ctrl+C to escape the watch, and\nthen test your deployment.\n\nVerify the domain URL for service. Should be helloworld-go.default.example.com.\n\nkubectl get kservice helloworld-go \\\n  --namespace default \\\n  --output=custom-columns=NAME:.metadata.name,DOMAIN:.status.domain\n\n\nAnd test your service.\n\ncurl --connect-to helloworld-go.default.example.com:80:$(glooctl proxy address --name clusteringress-proxy) http://helloworld-go.default.example.com\n\n\nCleanup\n\nkubectl delete \\\n  --filename serviceaccount.yaml \\\n  --filename service-build.yaml\n\nkubectl delete secret basic-user-pass\n\n\nSummary\n\nHopefully, this post gave you a taste for how Gloo and Knative can work together to provide you with a way to build and\ndeploy your services on demand into Kubernetes.\n\nSee Also\n\n\n  https://github.com/knative/docs/blob/master/install/getting-started-knative-app.md\n  https://github.com/knative/docs\n  https://gloo.solo.io/getting_started/kubernetes/gloo_with_knative/\n\n",
      tags: ["Gloo","Knative","Kubernetes"],
      id: 1
    });
    

    index.add({
      title: "Kubernetes Ingress Control using Gloo",
      category: null,
      content: "Kubernetes is excellent and makes it easier to create and manage highly distributed applications. A challenge then is how do you share your great Kubernetes hosted applications with the rest of the world. Many lean towards Kubernetes Ingress objects and this article will show you how to use the open source Solo.io Gloo to fill this need.\n\n\n\nGloo is a function gateway that gives users many benefits including sophisticated function level routing, and extensive service discovery with the introspection of OpenAPI (Swagger) definitions, gRPC reflection, Lambda discovery and more. Gloo can act as an Ingress Controller, that is, by routing Kubernetes external traffic to Kubernetes cluster hosted services based on the path routing rules defined in an Ingress Object. I’m a big believer in showing technology through examples, so let’s quickly run through an example to show you what’s possible.\n\nPrerequisites\n\nThis example assumes you’re running on a local minikube instance, and that you also have kubectl also running. You can run this same example on your favorite cloud provider managed Kubernetes cluster; you’d need to make a few tweaks. You’ll also need Gloo installed. Let’s use Homebrew to set all of this up for us, and then start minikube and install Gloo. It will take a few minutes to download and install everything to your local machine, and get everything started.\n\nbrew update\nbrew cask install minikube\nbrew install kubectl glooctl curl\n\nminikube start\nglooctl install ingress\n\n\nOne more thing before we dive into Ingress objects, let’s set up an example service deployed on Kubernetes that we can reference.\n\nkubectl apply \\\n  --filename https://raw.githubusercontent.com/solo-io/gloo/master/example/petstore/petstore.yaml\n\n\nSetting up an Ingress to our example Petstore\n\nLet’s set up an Ingress object that routes all HTTP traffic to our petstore service. To make this a little more exciting and challenging, and who doesn’t like a good tech challenge, let’s also configure a host domain, which will require a little extra curl magic to call correctly on our local Kubernetes cluster. The following Ingress definition will route all requests to http://gloo.example.com to our petstore service listening on port 8080 within our cluster. The petstore service provides some REST functions listening on the query path /api/pets that will return JSON for the inventory of pets in our (small) store.\n\nIf you are trying this example in a public cloud Kubernetes instance, you’ll most likely need to configure a Cloud Load Balancer. Make sure you configure that Load Balancer for the service/ingress-proxy running in the gloo-system namespace.\n\nThe important details of our example Ingress definition are:\n\n\n  Annotation kubernetes.io/ingress.class: gloo which is the standard way to mark an Ingress object as handled by a specific Ingress controller, i.e., Gloo. This requirement will go away soon as we add the ability for Gloo to be the cluster default Ingress controller\n  Path wildcard /.* to indicate that all traffic to http://gloo.example.com is routed to our petstore service\n\n\ncat &lt;&lt;EOF | kubectl apply --filename -\napiVersion: extensions/v1beta1\nkind: Ingress\nmetadata:\n name: petstore-ingress\n annotations:\n    kubernetes.io/ingress.class: gloo\nspec:\n  rules:\n  - host: gloo.example.com\n    http:\n      paths:\n      - path: /.*\n        backend:\n          serviceName: petstore\n          servicePort: 8080\nEOF\n\n\nWe can validate that Kubernetes created our Ingress correctly by the following command.\n\nkubectl get ingress petstore-ingress\n\nNAME               HOSTS              ADDRESS   PORTS   AGE\npetstore-ingress   gloo.example.com             80      14h\n\n\nTo test we’ll use curl to call our local cluster. Like I said earlier, by defining a host: gloo.example.com in our Ingress, we need to do a little more to call this without doing things with DNS or our local /etc/hosts file. I’m going to use the recent curl --connect-to options, and you can read more about that at the curl man pages.\n\nThe glooctl command-line tool helps us get the local host IP and port for the proxy with the glooctl proxy address --name &lt;ingress name&gt; --port http command. It returns the address (host IP:port) to the Gloo Ingress proxy that allows us external access to our local Kuberbetes cluster. If you are trying this example in a public cloud managed Kuberbetes, then most will handle the DNS mapping for your specified domain (that you should own), and the Gloo Ingress service, so in that case, you do NOT need the --connect-to magic, just curl http://gloo.example.com/api/pets should work.\n\ncurl --connect-to gloo.example.com:80:$(glooctl proxy address --name ingress-proxy --port http) \\\n    http://gloo.example.com/api/pets\n\n\nWhich should return the following JSON\n\n[{\"id\":1,\"name\":\"Dog\",\"status\":\"available\"},{\"id\":2,\"name\":\"Cat\",\"status\":\"pending\"}]\n\n\nTLS Configuration\n\nThese days, most want to use TLS to secure your communications. Gloo Ingress can act as a TLS terminator, and we’ll quickly run through what that set up would look like.\n\nAny Kubernetes Ingress doing TLS will need a Kubernetes TLS secret created, so let’s create a self-signed certificate we can use for our example gloo.example.com domain. The following two commands will produce a certificate and generate a TLS secret named my-tls-secret in minikube.\n\nopenssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout my_key.key -out my_cert.cert -subj \"/CN=gloo.example.com/O=gloo.example.com\"\n\nkubectl create secret tls my-tls-secret --key my_key.key --cert my_cert.cert\n\n\nNow let’s update our Ingress object with the needed TLS configuration. Important that the TLS host and the rules host match, and the secretName matches the name of the Kubernetes secret deployed previously.\n\ncat &lt;&lt;EOF | kubectl apply --filename -\napiVersion: extensions/v1beta1\nkind: Ingress\nmetadata:\n  name: petstore-ingress\n  annotations:\n    kubernetes.io/ingress.class: gloo\nspec:\n  tls:\n  - hosts:\n    - gloo.example.com\n    secretName: my-tls-secret\n  rules:\n  - host: gloo.example.com\n    http:\n      paths:\n      - path: /.*\n        backend:\n          serviceName: petstore\n          servicePort: 8080\nEOF\n\n\nIf all went well, we should have changed our petstore to now be listening to https://gloo.example.com. Let’s try it, again using our curl magic, which we need to both resolve the host and port as well as to validate our certificate. Notice that we’re asking glooctl for --port https this time, and we’re curling https://gloo.example.com on port 443. We’ll also have curl validate our TLS certificate using curl --cacert &lt;my_cert.cert&gt; with the certificate we created and used in our Kubernetes secret.\n\ncurl --cacert my_cert.cert \\\n    --connect-to gloo.example.com:443:$(glooctl proxy address --name ingress-proxy --port https) \\\n    https://gloo.example.com/api/pets\n\n\nNext Steps\n\nThis was a quick tour of how Gloo can act as your Kubernetes Ingress controller making very minimal changes to your existing Kubernetes manifests. Please try it out and let us know what you think at our community Slack channel.\n\nIf you’re interested in powering up you Gloo superpowers, try Gloo in gateway mode glooctl install gateway, which unlocks a set of Kubernetes CRDs (Custom Resources) that give you a more standard, and far more powerful, way of doing more advanced traffic shifting, rate limiting, and more without the annotation smell in your Kubernetes cluster. Check out these other articles for more details on Gloo’s extra powers.\n\n\n  Routing with Gloo Function Gateway\n  Canary Deployments with Gloo Function Gateway\n  Canary Deployments with Gloo Function Gateway using Weighted Destinations\n\n",
      tags: ["Gloo","Ingress","Kubernetes"],
      id: 2
    });
    

    index.add({
      title: "Canary Deployments with Gloo Function Gateway using Weighted Destinations",
      category: null,
      content: "\n    \n    Photo by Form.\n\n\nThis is the 3rd post in my 3 part series on doing Canary Releases with Solo.io Gloo.\n\n\n  In part 1, Routing with Gloo Function Gateway,\nyou learned how to setup Gloo, and using Gloo to setup some initial function level routing rules.\n  In part 2, Canary Deployments with Gloo Function Gateway,\nyou learned how to setup a conditional routing rule that routed requests to the new version of our service only when\na request header was present with the correct value.\n  In part 3, Canary Deployments with Gloo Function Gateway using Weighted Destinations,\nwe used weighted destinations to route a percentage of request traffic to individual upstream services.\n\n\nThis post will show a different way of doing Canary release by using weighted routes to send a fraction of the request\ntraffic to the new version (the canary). For example, you could initially route 5% of your request traffic to your new\nversion to validate that its working correctly in production without risking too much if your new version fails. As you\ngain confidence in your new version, you can route more and more traffic to it until you cut over completely, i.e. 100%\nto new version, and decommission the old version.\n\nAll of the Kubernetes manifests are located at https://github.com/scranton/gloo-canary-example. I’d suggest you clone\nthat repo locally to make it easier to try these example yourself. All command examples assume you are in the top level\ndirectory of that repo.\n\nReview\n\nQuickly reviewing the previous 2 posts, we learned that Gloo can help with function level routing, and that routing can\nbe used as part of a Canary Release process, that is slowly testing a new version of our service in an environment. In\nthe last post, we used Gloo to create a special routing rule to our new version that only forwarded on requests that\nincluded a specific request header. That allows us to deploy our new service into production, while only allowing\nrequest traffic from specific clients, i.e. clients that know to set that specific request header. Once we got\nconfident that our new version was working as expected, we then changed the Gloo routing rules so that all request\ntraffic went to the new service. This is a great way to validate that our new deployment is correctly configured in our\nenvironment before sending any important traffic to it.\n\nIn this post, we’re going to expand on that approach with a more sophisticated pattern - weighted routes. With this\ncapability we can route a percentage of the request traffic to one or more functions. This enhances our previous header\nbased approach as we can now validate that our new service can handle a managed load of traffic, and as we gain\nconfidence we can route higher loads to the new version till its handling 100% of the request traffic. If at any point,\nwe see errors we can either rollback 100% of traffic to the original working version OR debug our service to better\nunderstand why it started to have problems handling a faction of our target load, which in theory should help us fix\nour new service version quicker.\n\nYou can always combine both the header routing and weighted destination routing, and other routing options Gloo provides.\n\nSetup\n\nThis post assumes you’ve already run thru the Canary Deployments with Gloo Function Gateway\npost, and that you’ve already got a Kubernetes environment setup with Gloo. If not, please refer back to that post for\nsetup instructions and the basics of VirtualServices and Routes with Gloo.\n\nBy the end of that post, we had 100% of findPets function traffic going to our petstore-v2 service, and the other\nfunctions going to the original petstore-v1. Let’s validate our services before we make any changes.\n\nexport PROXY_URL=$(glooctl proxy url)\ncurl ${PROXY_URL}/findPets\n\n\nThe call to findPets should have been routed to petstore-v2, which should return the following result.\n\n[{\"id\":1,\"name\":\"Dog\",\"status\":\"v2\"},{\"id\":2,\"name\":\"Cat\",\"status\":\"v2\"},{\"id\":3,\"name\":\"Parrot\",\"status\":\"v2\"}]\n\n\nAnd calls to findPetWithId should route to petstore-v1, which only has 2 pets (Dog &amp; Cat) each with a status of\navailable and pending respectively (versus status of v2 for petstore-v2 responses).\n\ncurl ${PROXY_URL}/findPetWithId/1\n\n\n{\"id\":1,\"name\":\"Dog\",\"status\":\"available\"}\n\n\ncurl ${PROXY_URL}/findPetWithId/2\n\n\n{\"id\":2,\"name\":\"Cat\",\"status\":\"pending\"}\n\n\ncurl ${PROXY_URL}/findPetWithId/3\n\n\n{\"code\":404,\"message\":\"not found: pet 3\"}\n\n\nSo let’s play with doing a Canary Release with weighted destinations to migrate the findPetWithId function.\n\nSetting up Weighted Destinations in Gloo\n\nLet’s start by looking at our existing, virtual service coalmine\n\nkubectl get virtualservice coalmine --namespace gloo-system --output yaml\n\n\napiVersion: gateway.solo.io/v1\nkind: VirtualService\nmetadata:\n  name: coalmine\n  namespace: gloo-system\nspec:\n  displayName: coalmine\n  virtualHost:\n    domains:\n    - '*'\n    name: gloo-system.coalmine\n    routes:\n    - matcher:\n        prefix: /findPets\n      routeAction:\n        single:\n          destinationSpec:\n            rest:\n              functionName: findPets\n              parameters: {}\n          upstream:\n            name: default-petstore-v2-8080\n            namespace: gloo-system\n    - matcher:\n        prefix: /findPetWithId\n      routeAction:\n        single:\n          destinationSpec:\n            rest:\n              functionName: findPetById\n              parameters:\n                headers:\n                  :path: /findPetWithId/{id}\n          upstream:\n            name: default-petstore-v1-8080\n            namespace: gloo-system\n    - matcher:\n        prefix: /petstore\n      routeAction:\n        single:\n          upstream:\n            name: default-petstore-v1-8080\n            namespace: gloo-system\n      routePlugins:\n        prefixRewrite:\n          prefixRewrite: /api/pets/\n\n\nTo create a weighted destination, we need to change the routeAction from single to multi\nand provide 2+ destination, which are destinationSpec with weight. For example, to route\n10% of request traffic to findPetWithId to petstore-v2 and the remaining 90% to petstore-v1.\n\nkubectl apply -f coalmine-virtual-service-part-3-weighted.yaml\n\n\nHere’s the relevant part of the virtual service manifest showing the weighted destination spec.\n\n\n  \n    This Github Sample is by scranton\n  \n  \n    coalmine-virtual-service-part-3-weighted.yaml view raw\n  \n\n\n  routeAction:\n    multi:\n      destinations:\n      - destination:\n          destinationSpec:\n            rest:\n              functionName: findPetById\n              parameters:\n                headers:\n                  :path: /findPetWithId/{id}\n          upstream:\n            name: default-petstore-v2-8080\n            namespace: gloo-system\n        weight: 10\n      - destination:\n          destinationSpec:\n            rest:\n              functionName: findPetById\n              parameters:\n                headers:\n                  :path: /findPetWithId/{id}\n          upstream:\n            name: default-petstore-v1-8080\n            namespace: gloo-system\n        weight: 90\n- matcher:\n\nLet’s run a shell loop to test, remember that petstore-v2 responses have a status field of v2. The following\ncommand will call our function 20 times, and we should see ~2 responses (~10%) return with \"status\":\"v2\".\n\nCOUNTER=0\nwhile [ $COUNTER -lt 20 ]; do\n    curl ${PROXY_URL}/findPetWithId/1\n    let COUNTER=COUNTER+1\ndone\n\n\n{\"id\":1,\"name\":\"Dog\",\"status\":\"available\"}\n{\"id\":1,\"name\":\"Dog\",\"status\":\"available\"}\n{\"id\":1,\"name\":\"Dog\",\"status\":\"available\"}\n{\"id\":1,\"name\":\"Dog\",\"status\":\"available\"}\n{\"id\":1,\"name\":\"Dog\",\"status\":\"available\"}\n{\"id\":1,\"name\":\"Dog\",\"status\":\"available\"}\n{\"id\":1,\"name\":\"Dog\",\"status\":\"available\"}\n{\"id\":1,\"name\":\"Dog\",\"status\":\"available\"}\n{\"id\":1,\"name\":\"Dog\",\"status\":\"v2\"}\n{\"id\":1,\"name\":\"Dog\",\"status\":\"available\"}\n{\"id\":1,\"name\":\"Dog\",\"status\":\"available\"}\n{\"id\":1,\"name\":\"Dog\",\"status\":\"available\"}\n{\"id\":1,\"name\":\"Dog\",\"status\":\"available\"}\n{\"id\":1,\"name\":\"Dog\",\"status\":\"v2\"}\n{\"id\":1,\"name\":\"Dog\",\"status\":\"available\"}\n{\"id\":1,\"name\":\"Dog\",\"status\":\"available\"}\n{\"id\":1,\"name\":\"Dog\",\"status\":\"available\"}\n{\"id\":1,\"name\":\"Dog\",\"status\":\"available\"}\n{\"id\":1,\"name\":\"Dog\",\"status\":\"available\"}\n{\"id\":1,\"name\":\"Dog\",\"status\":\"available\"}\n\n\nNow if we want to increase the traffic to our new version, we just need to update the weight attibutes in the 2\ndestination objects. Gloo sums all of the weight values within a given weighted destination route, and routes the\nrespective percentage to each destination. So if we set both route weights to 1 then each route would get 1/2 or 50%\nof the request traffic. I’d recommend setting the values with a sum of 100 so they look like percentages for greater\nreadability. The following example will update our routes to do 50/50 traffic split.\n\nkubectl apply -f coalmine-virtual-service-part-3-weighted-50-50.yaml\n\n\n\n  \n    This Github Sample is by scranton\n  \n  \n    coalmine-virtual-service-part-3-weighted-50-50.yaml view raw\n  \n\n\n  routeAction:\n    multi:\n      destinations:\n      - destination:\n          destinationSpec:\n            rest:\n              functionName: findPetById\n              parameters:\n                headers:\n                  :path: /findPetWithId/{id}\n          upstream:\n            name: default-petstore-v2-8080\n            namespace: gloo-system\n        weight: 50\n      - destination:\n          destinationSpec:\n            rest:\n              functionName: findPetById\n              parameters:\n                headers:\n                  :path: /findPetWithId/{id}\n          upstream:\n            name: default-petstore-v1-8080\n            namespace: gloo-system\n        weight: 50\n- matcher:\n\nAnd if we run our test loop again, we should see about 10 of the 20 requests returning \"status\":\"v2.\n\nCOUNTER=0\nwhile [ $COUNTER -lt 20 ]; do\n    curl ${PROXY_URL}/findPetWithId/1\n    let COUNTER=COUNTER+1\ndone\n\n\n{\"id\":1,\"name\":\"Dog\",\"status\":\"v2\"}\n{\"id\":1,\"name\":\"Dog\",\"status\":\"v2\"}\n{\"id\":1,\"name\":\"Dog\",\"status\":\"v2\"}\n{\"id\":1,\"name\":\"Dog\",\"status\":\"available\"}\n{\"id\":1,\"name\":\"Dog\",\"status\":\"v2\"}\n{\"id\":1,\"name\":\"Dog\",\"status\":\"available\"}\n{\"id\":1,\"name\":\"Dog\",\"status\":\"available\"}\n{\"id\":1,\"name\":\"Dog\",\"status\":\"v2\"}\n{\"id\":1,\"name\":\"Dog\",\"status\":\"v2\"}\n{\"id\":1,\"name\":\"Dog\",\"status\":\"available\"}\n{\"id\":1,\"name\":\"Dog\",\"status\":\"v2\"}\n{\"id\":1,\"name\":\"Dog\",\"status\":\"v2\"}\n{\"id\":1,\"name\":\"Dog\",\"status\":\"available\"}\n{\"id\":1,\"name\":\"Dog\",\"status\":\"available\"}\n{\"id\":1,\"name\":\"Dog\",\"status\":\"v2\"}\n{\"id\":1,\"name\":\"Dog\",\"status\":\"v2\"}\n{\"id\":1,\"name\":\"Dog\",\"status\":\"available\"}\n{\"id\":1,\"name\":\"Dog\",\"status\":\"available\"}\n{\"id\":1,\"name\":\"Dog\",\"status\":\"available\"}\n{\"id\":1,\"name\":\"Dog\",\"status\":\"v2\"}\n\n\nSummary\n\nThis series has hopefully given you all a taste of how Solo.io Gloo can help\nyou create more interesting applications, and also enhance your application delivery approaches. These posts have shown\nhow to do function level request routing, and how you can enhance those routing rules by requiring presence of request\nheaders and doing managed load balancing by specifying the percentage of traffic going to individual upstream\ndestinations. Gloo supports many more options, and I hope you’ll continue your journey by going to https://gloo.solo.io\nto learn more.\n",
      tags: ["Gloo","Canary"],
      id: 3
    });
    

    index.add({
      title: "Canary Deployments with Gloo Function Gateway",
      category: null,
      content: "\n    \n    Photo by Zab Consulting.\n\n\nThis is the 2nd post in my 3 part series on doing Canary Releases with Solo.io Gloo.\n\n\n  In part 1, Routing with Gloo Function Gateway,\nyou learned how to setup Gloo, and using Gloo to setup some initial function level routing rules.\n  In part 2, Canary Deployments with Gloo Function Gateway,\nyou learned how to setup a conditional routing rule that routed requests to the new version of our service only when\na request header was present with the correct value.\n  In part 3, Canary Deployments with Gloo Function Gateway using Weighted Destinations,\nwe used weighted destinations to route a percentage of request traffic to individual upstream services.\n\n\nThis post expands on the Function Routing with Gloo\npost to show you how to do a Canary release of a new version of a function. Gloo is a function gateway\nthat gives users a number of benefits including sophisticated function level routing, and deep service discovery with\nintrospection of OpenAPI (Swagger) definitions, gRPC reflection, Lambda discovery and more. This post will show a simple\nexample of Gloo discovering 2 different deployments of a service, and setting up some routes. The route rules will use the\npresence of a request header x-canary:true to influence runtime routing to either version 1 or version 2 of our function.\nThen once we’re happy with our new version, we will update the route so all requests now go to version 2 of our\nservice. All without changing or even redeploying our 2 services. But first, let’s set some context…\n\nBackground\n\n\n  \n    Canary release is a technique to reduce the risk of introducing a new software version in production by slowly\n    rolling out the change to a small subset of users before rolling it out to the entire infrastructure and making it\n    available to everybody.\n  \n  \n    Danilo Sato\n    Canary Release\n  \n\n\nThe idea of a Canary release is that no matter how much testing you do on a new implementation, until you deploy it into\nyour production environment you can’t be positive everything will work as expected. So having a way to release a new\nversion into production concurrently with the existing version(s) with some way to route traffic can be helpful. Ideally,\nwe’d like to route most traffic to existing, known to work version, and have a way for some (test) requests go to the\nnew version. Once you’re feeling comfortable that your new version is working like you expect, then and only then, do you start\nrouting most/all requests to the new version, and then eventually decommission the original service.\n\nBeing able to change request routes without needing to change or redeploy your code, I think, is very helpful in\nbuilding confidence that your code is ready for production. That is, if you need to change your code or use a code based\nfeature flag, then your exercising different code paths and/or changing deployed configuration settings. I feel its\nbetter if you can deploy your service, code and configurations, all ready for production, and use an external mechanism\nto manage request routing.\n\nGloo uses Envoy, which is a super high performance service proxy, to do the request routing.\nIn this example, we’ll use a request header to influence the routing, though we could also use other variables like the\nIP range of the requestor to drive routing decisions. That is, if requests are coming from specific test machines we can\nroute them to our new version. Lots more information on how Gloo and Envoy works can be found on the Solo.io\nwebsite. On to the example…\n\nThis post assumes you’ve already run thru the Function Routing with Gloo\npost, and that you’ve already got a Kubernetes environment setup with Gloo. If not, please refer back to that post for\nsetup instructions and the basics of VirtualServices and Routes with Gloo.\n\nAll of the Kubernetes manifests are located at https://github.com/scranton/gloo-canary-example. I’d suggest you clone\nthat repo locally to make it easier to try these example yourself. All command examples assume you are in the top level\ndirectory of that repo.\n\nReview\n\nIn the previous post, we had a single service petstore-v1, and we setup Gloo to route requests to its findPets\nREST function. Let’s test that its still working as expected. Remember we need to get Gloo’s proxy url by calling the\nglooctl proxy url command, and then we can make requests against that with the /findPets route that we previously\nsetup. If still working correctly we should get 2 Pets back.\n\nexport PROXY_URL=$(glooctl proxy url)\ncurl ${PROXY_URL}/findPets\n\n\n[{\"id\":1,\"name\":\"Dog\",\"status\":\"available\"},{\"id\":2,\"name\":\"Cat\",\"status\":\"pending\"}]\n\n\nCanary Routing\n\nNow let’s deploy a version 2 of our service, and let’s setup a canary route for the findPets function. That is, by\ndefault we’ll route to version 1 of the function, and if there is a request header x-canary:true set, we’ll route that\nrequest to version 2 of our function.\n\nInstall and verify petstore version 2 example service\n\nLet’s first deploy version 2 of our petstore service. This version has been modified to return 3 pets.\n\nkubectl apply -f petstore-v2.yaml\n\n\n\n  \n    This Github Sample is by scranton\n  \n  \n    petstore-v2.yaml view raw\n  \n\n\n---\n# petstore-v2\napiVersion: v1\nkind: Service\nmetadata:\n name: petstore-v2\n namespace: default\n labels:\n   app: petstore-v2\nspec:\n type: ClusterIP\n ports:\n - name: http\n   port: 8080\n   targetPort: 8080\n   protocol: TCP\n selector:\n   app: petstore-v2\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n name: petstore-v2\n namespace: default\n labels:\n   app: petstore-v2\nspec:\n replicas: 1\n selector:\n   matchLabels:\n     app: petstore-v2\n template:\n   metadata:\n     labels:\n       app: petstore-v2\n   spec:\n     containers:\n     - name: petstore-v2\n       image: scottcranton/petstore:v2\n       ports:\n       - containerPort: 8080\n\nVerify its setup right\n\nkubectl get services --namespace default\n\n\nNAME          TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE\nkubernetes    ClusterIP   10.96.0.1       &lt;none&gt;        443/TCP    22h\npetstore-v1   ClusterIP   10.110.99.86    &lt;none&gt;        8080/TCP   33m\npetstore-v2   ClusterIP   10.109.91.120   &lt;none&gt;        8080/TCP   6s\n\n\nNow let’s setup a port forward to see if it works. When we do a GET against /api/pets we should get back 3 pets.\n\nkubectl port-forward services/petstore-v2 8080:8080\n\n\nAnd in a different terminal, run the following to see if we get back 3 pets for version 2 of our service.\n\ncurl localhost:8080/api/pets\n\n\n[{\"id\":1,\"name\":\"Dog\",\"status\":\"v2\"},{\"id\":2,\"name\":\"Cat\",\"status\":\"v2\"},{\"id\":3,\"name\":\"Parrot\",\"status\":\"v2\"}]\n\n\nYou should kill all port forwarding as we’ll use Gloo to proxy future tests.\n\nSetup Canary Route\n\nLet’s setup a new function route rule for petstore version 2 findPets function that depends on the presence of the\nx-canary:true request header.\n\nglooctl add route \\\n   --name coalmine \\\n   --path-prefix /findPets \\\n   --dest-name default-petstore-v2-8080 \\\n   --rest-function-name findPets \\\n   --header x-canary=true\n\n\nDefault routing should still go to petstore version 1, and return only 2 pets.\n\ncurl ${PROXY_URL}/findPets\n\n\n[{\"id\":1,\"name\":\"Dog\",\"status\":\"available\"},{\"id\":2,\"name\":\"Cat\",\"status\":\"pending\"}]\n\n\nIf we make a request with the x-canary:true set, it should route to petstore version 2, and return 3 pets.\n\ncurl -H \"x-canary:true\" ${PROXY_URL}/findPets\n\n\n[{\"id\":1,\"name\":\"Dog\",\"status\":\"v2\"},{\"id\":2,\"name\":\"Cat\",\"status\":\"v2\"},{\"id\":3,\"name\":\"Parrot\",\"status\":\"v2\"}]\n\n\nJust to verify, let’s set the header to a different value, e.g. x-canary:false to see that it routes to petstore v1\n\ncurl -H \"x-canary:false\" ${PROXY_URL}/findPets\n\n\n[{\"id\":1,\"name\":\"Dog\",\"status\":\"available\"},{\"id\":2,\"name\":\"Cat\",\"status\":\"pending\"}]\n\n\nHere’s the complete YAML for our coalmine virtual service that you could kubectl apply if you wanted to recreate\n\n\n  \n    This Github Sample is by scranton\n  \n  \n    coalmine-virtual-service-part-2-header.yaml view raw\n  \n\n\napiVersion: gateway.solo.io/v1\nkind: VirtualService\nmetadata:\n  name: coalmine\n  namespace: gloo-system\nspec:\n  displayName: coalmine\n  virtualHost:\n    domains:\n    - '*'\n    name: gloo-system.coalmine\n    routes:\n    - matcher:\n        headers:\n        - name: x-canary\n          regex: true\n          value: \"true\"\n        prefix: /findPets\n      routeAction:\n        single:\n          destinationSpec:\n            rest:\n              functionName: findPets\n              parameters: {}\n          upstream:\n            name: default-petstore-v2-8080\n            namespace: gloo-system\n    - matcher:\n        prefix: /findPetWithId\n      routeAction:\n        single:\n          destinationSpec:\n            rest:\n              functionName: findPetById\n              parameters:\n                headers:\n                  :path: /findPetWithId/{id}\n          upstream:\n            name: default-petstore-v1-8080\n            namespace: gloo-system\n    - matcher:\n        prefix: /findPets\n      routeAction:\n        single:\n          destinationSpec:\n            rest:\n              functionName: findPets\n              parameters: {}\n          upstream:\n            name: default-petstore-v1-8080\n            namespace: gloo-system\n    - matcher:\n        prefix: /petstore\n      routeAction:\n        single:\n          upstream:\n            name: default-petstore-v1-8080\n            namespace: gloo-system\n      routePlugins:\n        prefixRewrite:\n          prefixRewrite: /api/pets\n\nThe part of the virtual service manifest that is specifying the header based routing is highlighted as follows.\n\n- matcher:\n    headers:\n    - name: x-canary\n      regex: true\n      value: \"true\"\n    prefix: /findPets\n  routeAction:\n\nMake version 2 the default for all requests\n\nOnce we’re feeling good about version 2 of our function, we can make the default call to /findPets go to version 2.\nNote that will Gloo as your function gateway, you do not have to route all function requests to version 2 of the petstore\nservice. In this example, we’re only routing requests for the findPets function to version 2. All other requests are\ngoing to version 1 of petstore. This partial routing may not always work for all services; this post is showing that\nGloo makes this level of granularity possible when it helps you more fine tune your application upgrading decisions. For\nexample, this may make sense if you want to patch a critical bug but are not ready to role out other breaking changes in\na new service version.\n\nThe easiest way to make change the routing rules to route all requests to version 2 findPets is by applying a YAML\nfile. You can use the glooctl command line tool to add and remove routes, but it takes several calls.\n\n\n  \n    This Github Sample is by scranton\n  \n  \n    coalmine-virtual-service-part-2-v2.yaml view raw\n  \n\n\napiVersion: gateway.solo.io/v1\nkind: VirtualService\nmetadata:\n  name: coalmine\n  namespace: gloo-system\nspec:\n  displayName: coalmine\n  virtualHost:\n    domains:\n    - '*'\n    name: gloo-system.coalmine\n    routes:\n    - matcher:\n        prefix: /findPets\n      routeAction:\n        single:\n          destinationSpec:\n            rest:\n              functionName: findPets\n              parameters: {}\n          upstream:\n            name: default-petstore-v2-8080\n            namespace: gloo-system\n    - matcher:\n        prefix: /findPetWithId\n      routeAction:\n        single:\n          destinationSpec:\n            rest:\n              functionName: findPetById\n              parameters:\n                headers:\n                  :path: /findPetWithId/{id}\n          upstream:\n            name: default-petstore-v1-8080\n            namespace: gloo-system\n    - matcher:\n        prefix: /petstore\n      routeAction:\n        single:\n          upstream:\n            name: default-petstore-v1-8080\n            namespace: gloo-system\n      routePlugins:\n        prefixRewrite:\n          prefixRewrite: /api/pets\n\nSummary\n\nThis post has shown you have to leverage the Gloo function gateway to do a Canary Release of a new version of a function,\nand allow you to do very granular function level routing to validate your new function is working correctly. Then it showed\nchanging routing rules so all traffic goes to the new version. All without redeploying either of the 2 service\nimplementations. In this post we used the presence of a request header to influence function routing; we could also have\ndone routing based on IP range of incoming request or other variables. This hopefully shows you the power and flexibility\nthat Gloo function gateway can provide you in your journey to microservices and service mesh.\n",
      tags: ["Gloo","Canary"],
      id: 4
    });
    

    index.add({
      title: "Routing with Gloo Function Gateway",
      category: null,
      content: "\n    \n    Photo by Pietro Jeng.\n\n\nThis is the 1st post in my 3 part series on doing Canary Releases with Solo.io Gloo.\n\n\n  In part 1, Routing with Gloo Function Gateway,\nyou learned how to setup Gloo, and using Gloo to setup some initial function level routing rules.\n  In part 2, Canary Deployments with Gloo Function Gateway,\nyou learned how to setup a conditional routing rule that routed requests to the new version of our service only when\na request header was present with the correct value.\n  In part 3, Canary Deployments with Gloo Function Gateway using Weighted Destinations,\nwe used weighted destinations to route a percentage of request traffic to individual upstream services.\n\n\nThis post introduces you to how to use the open source Solo.io Gloo\nproject to help you to route traffic to your Kubernetes hosted services. Gloo is a function gateway that gives users a number of benefits including sophisticated\nfunction level routing, and deep service discovery with introspection of OpenAPI (Swagger) definitions, gRPC reflection,\nLambda discovery and more. This post will start with a simple example of Gloo discovering and routing requests to a\nservice that exposes REST functions. Later posts will build on this initial example to do highlight ever more complex\nscenarios.\n\nPrerequisites\n\nI’m assuming you’re already running a Kubernetes installation. I’m assuming you’re using minikube\nfor this post though any recent Kubernetes installation should work as long as you have kubectl\nsetup and configured correctly for your Kubernetes installation.\n\nSetup\n\nSetup example service\n\nAll of the Kubernetes manifests are located at https://github.com/scranton/gloo-canary-example. I’d suggest you clone\nthat repo locally to make it easier to try these example yourself. All command examples assume you are in the top level\ndirectory of that repo.\n\nLet’s start by installing an example service that exposes 4 REST functions. This service is based on the\ngo-swagger petstore example.\n\nkubectl apply -f petstore-v1.yaml\n\n\n\n  \n    This Github Sample is by scranton\n  \n  \n    petstore-v1.yaml view raw\n  \n\n\n---\n# petstore-v1\napiVersion: v1\nkind: Service\nmetadata:\n  name: petstore-v1\n  namespace: default\n  labels:\n    app: petstore-v1\nspec:\n  type: ClusterIP\n  ports:\n  - name: http\n    port: 8080\n    targetPort: 8080\n    protocol: TCP\n  selector:\n    app: petstore-v1\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: petstore-v1\n  namespace: default\n  labels:\n    app: petstore-v1\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: petstore-v1\n  template:\n    metadata:\n      labels:\n        app: petstore-v1\n    spec:\n      containers:\n      - name: petstore-v1\n        image: scottcranton/petstore:v1\n        ports:\n        - containerPort: 8080\n\nWe’ve installing this service into the default namespace, so we can look there to see if it’s installed correctly.\n\nkubectl get all --namespace default\n\n\nNAME                              READY   STATUS    RESTARTS   AGE\npod/petstore-v1-986747fc8-6hn9p   1/1     Running   0          16s\n\nNAME                  TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)    AGE\nservice/kubernetes    ClusterIP   10.96.0.1      &lt;none&gt;        443/TCP    22h\nservice/petstore-v1   ClusterIP   10.110.99.86   &lt;none&gt;        8080/TCP   17s\n\nNAME                          READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/petstore-v1   1/1     1            1           16s\n\nNAME                                    DESIRED   CURRENT   READY   AGE\nreplicaset.apps/petstore-v1-986747fc8   1         1         1       16s\n\n\nLet’s test our service to make sure it installed correctly. This service is setup to expose on port 8080, and will\nreturn the list of all pets for GET requests on the query path /api/pets. The easiest way to test is to\nport-forward the service so we can access it locally. We’ll need the service name for the port forwarding. Make sure\nthe service name matches the ones from your system. This will forward port 8080 from the service running in your Kubernetes\ninstallation to your local machine, i.e. localhost:8080.\n\nGet the service names for your installation\n\nkubectl get service --namespace default\n\n\nNAME          TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)    AGE\nkubernetes    ClusterIP   10.96.0.1      &lt;none&gt;        443/TCP    22h\npetstore-v1   ClusterIP   10.110.99.86   &lt;none&gt;        8080/TCP   42s\n\n\nSetup the port forwarding\n\nkubectl port-forward service/petstore-v1 8080:8080\n\n\nIn a separate terminal, run the following. The petstore function should return 2 pets: Dog and Cat.\n\ncurl localhost:8080/api/pets\n\n\n[{\"id\":1,\"name\":\"Dog\",\"status\":\"available\"},{\"id\":2,\"name\":\"Cat\",\"status\":\"pending\"}]\n\n\nYou can also get the Swagger spec as well.\n\ncurl localhost:8080/swagger.json\n\n\nYou can kill all the port forwards. Now we’ll setup Gloo…\n\nSetup Gloo\n\nLet’s setup the Gloo command line utility. Full instructions are at the Gloo doc site.\nHere are the quick setup instructions.\n\nSetup the glooctl command line tool. This makes installation, upgrade, and operations of Gloo easier. Full installation\ninstructions are located on the https://gloo.solo.io site.\n\nIf you’re a Mac or Linux Homebrew user, I’d recommend installing as follows.\n\nbrew install glooctl\n\n\nNow let’s install Gloo into your Kubernetes installation.\n\nglooctl install gateway\n\n\nPretty easy, eh? Let’s verify that its installed and running correctly. Gloo by default creates and installs into the\ngloo-system namespace, so let’s look at everything running there.\n\nkubectl get all --namespace gloo-system\n\n\nAnd the output should look something like the following.\n\nNAME                                 READY   STATUS    RESTARTS   AGE\npod/discovery-66c865f9bc-h6v8f       1/1     Running   0          22h\npod/gateway-777cf4486c-8mzj5         1/1     Running   0          22h\npod/gateway-proxy-5f58774ccc-rcmdv   1/1     Running   0          22h\npod/gloo-5c6c4466f-ptc8v             1/1     Running   0          22h\n\nNAME                    TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)                      AGE\nservice/gateway-proxy   LoadBalancer   10.97.13.246    &lt;pending&gt;     80:31333/TCP,443:32470/TCP   22h\nservice/gloo            ClusterIP      10.104.80.219   &lt;none&gt;        9977/TCP                     22h\n\nNAME                            READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/discovery       1/1     1            1           22h\ndeployment.apps/gateway         1/1     1            1           22h\ndeployment.apps/gateway-proxy   1/1     1            1           22h\ndeployment.apps/gloo            1/1     1            1           22h\n\nNAME                                       DESIRED   CURRENT   READY   AGE\nreplicaset.apps/discovery-66c865f9bc       1         1         1       22h\nreplicaset.apps/gateway-777cf4486c         1         1         1       22h\nreplicaset.apps/gateway-proxy-5f58774ccc   1         1         1       22h\nreplicaset.apps/gloo-5c6c4466f             1         1         1       22h\n\n\nRouting\n\nUpstreams\n\nBefore we get into routing, let’s talk a little about the concept of Upstreams. Upstreams are the services that Gloo\nhas discovered automatically. Let’s look at the upstreams that Gloo has discovered in our Kubernetes cluster.\n\nglooctl get upstreams\n\n\nYou may see some different entries than what follows. It depends on your Kubernetes cluster, and what is running currently.\n\n+-------------------------------+------------+----------+------------------------------+\n|           UPSTREAM            |    TYPE    |  STATUS  |           DETAILS            |\n+-------------------------------+------------+----------+------------------------------+\n| default-kubernetes-443        | Kubernetes | Accepted | svc name:      kubernetes    |\n|                               |            |          | svc namespace: default       |\n|                               |            |          | port:          443           |\n|                               |            |          |                              |\n| default-petstore-v1-8080      | Kubernetes | Accepted | svc name:      petstore-v1   |\n|                               |            |          | svc namespace: default       |\n|                               |            |          | port:          8080          |\n|                               |            |          | REST service:                |\n|                               |            |          | functions:                   |\n|                               |            |          | - addPet                     |\n|                               |            |          | - deletePet                  |\n|                               |            |          | - findPetById                |\n|                               |            |          | - findPets                   |\n|                               |            |          |                              |\n| gloo-system-gateway-proxy-443 | Kubernetes | Accepted | svc name:      gateway-proxy |\n|                               |            |          | svc namespace: gloo-system   |\n|                               |            |          | port:          443           |\n|                               |            |          |                              |\n| gloo-system-gateway-proxy-80  | Kubernetes | Accepted | svc name:      gateway-proxy |\n|                               |            |          | svc namespace: gloo-system   |\n|                               |            |          | port:          80            |\n|                               |            |          |                              |\n| gloo-system-gloo-9977         | Kubernetes | Accepted | svc name:      gloo          |\n|                               |            |          | svc namespace: gloo-system   |\n|                               |            |          | port:          9977          |\n|                               |            |          |                              |\n| kube-system-kube-dns-53       | Kubernetes | Accepted | svc name:      kube-dns      |\n|                               |            |          | svc namespace: kube-system   |\n|                               |            |          | port:          53            |\n|                               |            |          |                              |\n+-------------------------------+------------+----------+------------------------------+\n\n\nNotice that our petstore service default-petstore-v1-8080 is different from the other upstreams in that its details is\nlisting 4 REST service functions: addPet, deletePet, findPetByID, and findPets. This is because Gloo can auto-detect\nOpenAPI / Swagger definitions. This allows Gloo to route to individual functions versus what most traditional API\nGateways do in only letting you route only to host:port granular services. Let’s see that in action.\n\nLet’s look a little closer at our petstore upstream. The glooctl command let’s us output the full details in yaml or\njson.\n\nglooctl get upstream default-petstore-v1-8080 --output yaml\n\n\ndiscoveryMetadata: {}\nmetadata:\n  annotations:\n    kubectl.kubernetes.io/last-applied-configuration: |\n      {\"apiVersion\":\"v1\",\"kind\":\"Service\",\"metadata\":{\"annotations\":{},\"labels\":{\"app\":\"petstore-v1\"},\"name\":\"petstore-v1\",\"namespace\":\"default\"},\"spec\":{\"ports\":[{\"name\":\"http\",\"port\":8080,\"protocol\":\"TCP\",\"targetPort\":8080}],\"selector\":{\"app\":\"petstore-v1\"},\"type\":\"ClusterIP\"}}\n  labels:\n    app: petstore-v1\n    discovered_by: kubernetesplugin\n  name: default-petstore-v1-8080\n  namespace: gloo-system\n  resourceVersion: \"20387\"\nstatus:\n  reportedBy: gloo\n  state: Accepted\nupstreamSpec:\n  kube:\n    selector:\n      app: petstore-v1\n    serviceName: petstore-v1\n    serviceNamespace: default\n    servicePort: 8080\n    serviceSpec:\n      rest:\n        swaggerInfo:\n          url: http://petstore-v1.default.svc.cluster.local:8080/swagger.json\n        transformations:\n          addPet:\n            body:\n              text: '{\"id\": {{ default(id, \"\") }},\"name\": \"{{ default(name, \"\")}}\",\"tag\":\n                \"{{ default(tag, \"\")}}\"}'\n            headers:\n              :method:\n                text: POST\n              :path:\n                text: /api/pets\n              content-type:\n                text: application/json\n          deletePet:\n            headers:\n              :method:\n                text: DELETE\n              :path:\n                text: /api/pets/{{ default(id, \"\") }}\n              content-type:\n                text: application/json\n          findPetById:\n            body: {}\n            headers:\n              :method:\n                text: GET\n              :path:\n                text: /api/pets/{{ default(id, \"\") }}\n              content-length:\n                text: \"0\"\n              content-type: {}\n              transfer-encoding: {}\n          findPets:\n            body: {}\n            headers:\n              :method:\n                text: GET\n              :path:\n                text: /api/pets?tags={{default(tags, \"\")}}&amp;limit={{default(limit,\n                  \"\")}}\n              content-length:\n                text: \"0\"\n              content-type: {}\n              transfer-encoding: {}\n\nHere we see that the findPets REST function is looking for requests on /api/pets, and findPetsById is\nlooking for requests on /api/pets/{id} where {id} is the id number of the single pet who’s details are to be returned.\n\nBasic Routing\n\nGloo acts like an (better) Kubernetes Ingress, which\nmeans it can allow requests from external to the Kubernetes cluster to access services running inside the cluster. Gloo\nuses a concept called VirtualService to setup routes to Kubernetes hosted services.\n\nThis post will show you how to configure Gloo using the command line tools, and I’ll explain a little of what’s happening\nwith each command. I’ll also include the YAML at the end of each step if you’d prefer to work in a purely declarative\nfashion (versus imperative commands).\n\nSetup VirtualService. This gives us a place to define a set of related routes. This won’t do much till we create\nsome routes in the next steps.\n\nglooctl create virtualservice --name coalmine\n\n\nHere’s the YAML that will create the same resource as the glooctl command we just ran. Note that by default the\nglooctl command creates resources in the gloo-system namespace.\n\napiVersion: gateway.solo.io/v1\nkind: VirtualService\nmetadata:\n  name: coalmine\n  namespace: gloo-system\nspec:\n  displayName: coalmine\n  virtualHost:\n    domains:\n    - '*'\n    name: gloo-system.coalmine\n\n\nCreate a route for all traffic to go to our service.\n\nglooctl add route \\\n  --name coalmine \\\n  --path-prefix /petstore \\\n  --dest-name default-petstore-v1-8080 \\\n  --prefix-rewrite /api/pets\n\n\nThis sets up a simple ingress route so that all requests going to the Gloo Proxy / URL are redirected to the\ndefault-petstore-v1-8080 service /api/pets. Let’s test it. To get the Gloo proxy host and port number (remember\nthat Gloo is acting like a Kubernetes Ingress), we need to call glooctl proxy url. Then let’s call the route path.\n\nexport PROXY_URL=$(glooctl proxy url)\ncurl ${PROXY_URL}/petstore\n\n\nAnd we should see the same results as when we called the port forwarded service.\n\n[{\"id\":1,\"name\":\"Dog\",\"status\":\"available\"},{\"id\":2,\"name\":\"Cat\",\"status\":\"pending\"}]\n\n\nHere’s the full YAML for the coal virtual service created so far.\n\napiVersion: gateway.solo.io/v1\nkind: VirtualService\nmetadata:\n  name: coalmine\n  namespace: gloo-system\nspec:\n  displayName: coalmine\n  virtualHost:\n    domains:\n    - '*'\n    name: gloo-system.coalmine\n    routes:\n    - matcher:\n        prefix: /petstore\n      routeAction:\n        single:\n          upstream:\n            name: default-petstore-v1-8080\n            namespace: gloo-system\n      routePlugins:\n        prefixRewrite:\n          prefixRewrite: /api/pets\n\n\nFunction Routing\n\nWould it be better if we could just route to the named REST function versus having to know the specifics of the query\npath (i.e. /api/pets) the service is expecting? Gloo can help us with that. Let’s setup a route to findPets REST\nfunction.\n\nglooctl add route \\\n   --name coalmine \\\n   --path-prefix /findPets \\\n   --dest-name default-petstore-v1-8080 \\\n   --rest-function-name findPets\n\n\nAnd test it. We should see the same results as the request to /petstore as both those examples were exercising the\nfindPets REST function in the petstore service. This also shows that Gloo allows you to create multiple routing rules\nfor the same REST functions, if you want.\n\ncurl ${PROXY_URL}/findPets\n\n\nIf we want to route to a function with parameters, we can do that too by telling Gloo how to find the id parameter.\nIn this case, it happens to be a path parameter, but it could come from other parts of the request.\n\nNote: We’re about to create a route with a different name findPetWithId than the function name findPetById\nit is routing to. Gloo allows you to setup routing rules for any prefix path to any function name.\n\nglooctl add route \\\n   --name coalmine \\\n   --path-prefix /findPetWithId \\\n   --dest-name default-petstore-v1-8080 \\\n   --rest-function-name findPetById \\\n   --rest-parameters ':path=/findPetWithId/{id}'\n\n\nLet’s look up the details for the pet with id 1\n\ncurl ${PROXY_URL}/findPetWithId/1\n\n\n{\"id\":1,\"name\":\"Dog\",\"status\":\"available\"}\n\n\nAnd the pet with id 2\n\ncurl ${PROXY_URL}/findPetWithId/2\n\n\n{\"id\":2,\"name\":\"Cat\",\"status\":\"pending\"}\n\n\nHere’s the complete YAML you could apply to get the same virtual service setup that we just did. To recreate this\nvirtual service you could just kubectl apply the following YAML.\n\n\n  \n    This Github Sample is by scranton\n  \n  \n    coalmine-virtual-service-part-1.yaml view raw\n  \n\n\napiVersion: gateway.solo.io/v1\nkind: VirtualService\nmetadata:\n  name: coalmine\n  namespace: gloo-system\nspec:\n  displayName: coalmine\n  virtualHost:\n    domains:\n    - '*'\n    name: gloo-system.coalmine\n    routes:\n    - matcher:\n        prefix: /findPetWithId\n      routeAction:\n        single:\n          destinationSpec:\n            rest:\n              functionName: findPetById\n              parameters:\n                headers:\n                  :path: /findPetWithId/{id}\n          upstream:\n            name: default-petstore-v1-8080\n            namespace: gloo-system\n    - matcher:\n        prefix: /findPets\n      routeAction:\n        single:\n          destinationSpec:\n            rest:\n              functionName: findPets\n              parameters: {}\n          upstream:\n            name: default-petstore-v1-8080\n            namespace: gloo-system\n    - matcher:\n        prefix: /petstore\n      routeAction:\n        single:\n          upstream:\n            name: default-petstore-v1-8080\n            namespace: gloo-system\n      routePlugins:\n        prefixRewrite:\n          prefixRewrite: /api/pets\n\nSummary\n\nThis post is just the beginning of our function gateway journey with Gloo. Hopefully its given you a taste of some\nmore sophisticated function level routing options that are available to you. I’ll try to follow up with more posts on\neven more options available to you.\n",
      tags: ["Gloo"],
      id: 5
    });
    

    index.add({
      title: "New Personal Blog Site",
      category: null,
      content: "\n    \n    Photo by Sushobhan Badhai.\n\n\nI finally got around to updating my blog site to use Jekyll hosted on GitHub.com.\nI’m still looking at other static web site generators like Gatsbyjs and Hugo.\n\nA lot has happened since the last time I posted. Currntly I’m working at Solo.io leading their\nemerging Customer Success team. Its a small, and very promising startup in the microservices gateway, and service mesh\nspace. I’m very excited to be there, especially as I’m getting back hands on with code and technology again. Yay!\n\nMy hope is to get back into blogging and writing, so hopefully you see new posts and such appearing here soon.\n",
      tags: [],
      id: 6
    });
    

    index.add({
      title: "socat is so cool...",
      category: null,
      content: "I was helping someone the other day figure out how to use Camel to create a TCP/IP proxy, and I was trying to figure out the best way to test. Hiram Chirino pointed me at the socat utility, which is so super geeky cool that I am inspired to post about it...I installed socat on my Mac using MacPorts&gt; sudo ports install socatThe full code is here on GitHub https://github.com/scranton/camel-example-tcpipproxy.The Camel route is straightforward. It uses Camel-Mina to both listen on port 5000 and to connect to a request-response (InOut) service on port 5001. When you run this route, you can use socat as both the back-end service (echo style) and as a client. To start it as a back-end echo service, which will start a TCP/IP listener on port 5001 and will echo all messages sent to it (that's what 'PIPE' does).&gt; socat PIPE TCP4-LISTEN:5001To use socat as a TCP/IP client, run&gt; socat - tcp:localhost:5001Now any text you type in the Terminal where you started the client will be echo'd back to you - after it goes to and from the socat echo service. You can test this by stopping the socat echo service and seeing how the client reacts.Back to the Camel proxy... To test, after you start the socat echo server, you'd start the Camel route&gt; mvn install camel:runThis will start the Camel route standalone (the full example can also be deployed within ServiceMix), with a TCP/IP listener on port 5000 and a connection to the echo service on port 5001. Now you can start the socat client, but this time against port 5000 (the Camel proxy)&gt; socat - tcp:localhost:5000Now your text messages will run through the Camel proxy before going to the back end echo service; the Camel route will log each message that it proxies.A quick example of a cool TCP/IP utility, and how to use the powerful Camel framework to proxy anything... Thanks for the tip Hiram!",
      tags: [],
      id: 7
    });
    

    index.add({
      title: "I got an article published at JDJ...",
      category: null,
      content: "OSGi: An Overview of Its Impact on the Software Lifecycle— OSGi technology brings a number of much needed benefits to the Java enterprise application market, and is disruptive in that it impacts the software development, deployment, and management practices of many organizations. OSGi impacts deployment given the shared, modular nature of OSGi, meaning application code must be written differently to capitalize on the benefits of OSGi. Equally important, application management processes need to be adjusted, given the highly shared nature of OSGi modules across many applications. This article provides a high-level overview of OSGi, and the impact this framework is having on the software lifecycle.",
      tags: [],
      id: 8
    });
    

    index.add({
      title: "ServiceMix 4 Example Project",
      category: null,
      content: "I've just pushed a new version of the \"Getting Started with ServiceMix 4\" example webinar project to GitHub (https://github.com/scranton/servicemix4-example-payment-service). My goal is that this example shows best practices for creating ServiceMix 4 projects with Camel. I'm also experimenting with GitHub…This project is an example of using OSGi, Camel, and ServiceMix together, specifically ServiceMix 4's NMR component. A number of things are shown within this project: * use of the Camel-NMR component for inter OSGi bundle communication * dynamic routing combining Camel's recipient list and the OSGi Service Registry * use of Camel's Content Based Router * multiple front-end proxies (WS and batch file) * bridging one way (fire and forget) messaging with request-response * and much more...The scenario is a payment transfer service where transfer requests can be made either through a WS (SOAP/HTTP) interface or through batch files. These transfer requests are routed to banking services that can come and go at runtime (i.e. new banks can be added and removed at runtime).This solution is a bit over-engineered, but the goal of this effort is to provide examples of best practices in creating applications using these technologies.My plan is over the next few days to post some blog entries on aspects of this project to explain things like: how to communicate between OSGi bundles with Camel, dynamic routing to OSGi bundles that are deployed at runtime, etc. Ultimately I'll get to updating the webinar to talk through this updated code.I'd suggest downloading the 1.0.0 version of this project (https://github.com/scranton/servicemix4-example-payment-service/archives/payment-service-1.0.0) and give it a try with ServiceMix 4.3.0-fuse-02-00 (http://fusesource.com/downloads/). Please feel free to submit ideas for how I could enhance this example; I'm thinking adding in transaction support spanning two bank services would be cool, for example.",
      tags: [],
      id: 9
    });
    

    index.add({
      title: "ActiveMQ Message Groups",
      category: null,
      content: "The JMS specification does not provide a lot of guidance about use of Message Groups. Property Name Type Set By Use  JMSXGroupID String Client The identity of the message group this message is part of  JMSXGroupSeq int Client The sequence number of this message within the group; the first message is 1, the second 2,…JMSXGroupID and JMSXGroupSeq are standard properties clients should use if they want to group messages. All providers must support them.and that's about it… clear, right???This post is about how to use Message Groups within ActiveMQ. The documentation on using Message Groups within ActiveMQ has gotten much better -&nbsp;http://activemq.apache.org/message-groups.html, but there are still some nuances that users should be aware of.First of, why should you care about Message Groups…Message Groups are used to ensure that one and only one message consumer is processing messages, in order, from a queue (point to point). The message producer controls which messages are in what group, if any, by setting the JMSXGroupID message property.When message order matters, its much easier to ensure correct processing if only one message consumer is receiving the messages. The problem is that this does not scale; if you have more messages than a single consumer can process in a timely fashion, you need to add additional consumers. When you have multiple consumers receiving messages, you lose the easy ability to process messages in order; messages will be dispatched to available consumers based on many different criteria: round robin, least load, and just the fastest consumer gets more messages. So using Message Groups allows you to specify that for all the messages in that group should be processed by one and only one consumer, while all the other messages (not in groups or in different groups) can be load balanced to other consumers.The classic example for Message Groups is a stock feed. For any given stock (IBM, MSFT, ORCL), a queue might receive messages for quotes, buy orders, sell orders, bids, etc. If you're trading a given stock, it would be important that your trading application see all of the messages for that stock in order so that you can make the best trade (not get out bid, not pay too much, buy when some one sells cheap, etc.). The challenge is that there are a lot of stocks, and a lot of messages per stock so one message consumer can not possible handle the load. Enter Message Groups. So if each message is put into a message group by stock ticker (e.g. Message Producer sets \"JMSXGroupID = APPL\" in the message's properties), then ActiveMQ will ensure that only one consumer will get all the messages for that group in order. Pretty cool…So how does this work for ActiveMQ…Message Groups are controlled by the Message Producer, and are as easy as simple setting a message propertyMessage message = session.createTextMessage(\"hey\");  message.setStringProperty(\"JMSXGroupID\", \"IBM_NASDAQ_20/4/05\");  producer.send(message);That's it. Now only one message consumer will get all messages in that group (i.e. JMSXGroupID = IBM_NASDAQ_20/4/05)Other things you can do with Message Groups…The other producer controlled feature of Message Groups is setting the Message Group sequence id - JMSXGroupSeq. ActiveMQ requires that valid sequence numbers must be greater than 0 (i.e. start with 1). ActiveMQ takes no special action based on the sequence number, and does not enforce that they increase, are unique, etc. Sequence numbers are there as a convenience for you the developer, and it is your responsibility to make them meaningful to your application.The only time ActiveMQ cares about sequence numbers is when they are negative (less than 0). If you set JMSXGroupSeq = −1 then ActiveMQ will close the Message Group. What does it mean for the group to be closed? Closing a Message Group means that any future messages in that Message Group could be dispatched to a different consumer. That is, its as though this was the first use of that Message Group, and ActiveMQ will assign the now re-opened (message in group after JMSXGroupSeq=-1 was sent) to one and only one consumer.So when could a Message Group get assigned to a different consumer…There are only two instances where a Message Group would get re-assigned to a different consumer:Message Group is explicitly closed (JMSXGroupSeq = −1)the original consumer goes away (consumer.close(), loss of network connectivity, consumer process dies, …)In either of these two scenarios, ActiveMQ will reassign all future messages in that Message Group to a new consumer which will exclusively get all future messages… even if the original consumer comes back on line…Note: there is no way to force ActiveMQ to use a specific consumer for a specific Message Group (short of there only being one consumer for that queue). So if a Message Group consumer, for example, loses network connectivity temporarily, then ActiveMQ will re-assign future messages in that group to one of the surviving consumers.When ActiveMQ assigns (or reassigns) a Message Group, the first message in that group delivered to a consumer will have a special Boolean header set - JMSXGroupFirstForConsumer = true. This is a special flag to let a consumer know that this is the first message of a group that has been assigned to that consumer. It can allow the consumer to flush internal caches or do initial setup work that might be need to process the messages in that group.Tips and Tricks…Since ActiveMQ will assign Message Groups to consumers based on which consumers are available, it can be helpful to tell ActiveMQ to wait for all the consumers to come on line before ActiveMQ starts dispatching messages, and making Message Group assignments (even load balancing). To do this, you can set a couple of Destination Policy entries for that queue: consumersBeforeDispathStarts and timeBeforeDispathStarts. Both of these destination properties can into existence as of ActiveMQ 5.3.  &lt;destinationPolicy&gt;    &lt;policyMap&gt;      &lt;policyEntries&gt;        &lt;policyEntry queue=\"&gt;\" consumersBeforeDispatchStarts=\"2\" timeBeforeDispatchStarts=\"2000\"/&gt;      &lt;/policyEntries&gt;    &lt;/policyMap&gt;  &lt;/destinationPolicy&gt;This policy tells ActiveMQ to wait either 2 seconds (2000ms) or until 2 consumers connect. You can set either option, both, or neither per destination, or use ActiveMQ destination wildcards like in the above example which will include all queue.The other thing to be aware of is managing a large number of Message Groups. By default, ActiveMQ uses a Hash map of Message Groups (MessageGroupHashBucketFactory) that is limited to less than 1024 unique Message Group names. If you plan on using more than 1024 Message Group IDs (JMSXGroupID), than you need to configure ActiveMQ to use a different implementation to manage the larger number of Message Group IDs.&lt;destinationPolicy&gt;    &lt;policyMap&gt;      &lt;policyEntries&gt;        &lt;policyEntry queue=\"&gt;\"&gt;          &lt;messageGroupMapFactory&gt;            &lt;simpleMessageGroupMapFactory/&gt;          &lt;/messageGroupMapFactory&gt;        &lt;/policyEntry&gt;      &lt;/policyEntries&gt;    &lt;/policyMap&gt;  &lt;/destinationPolicy&gt;Setting the above Destination Policy will allow ActiveMQ to manage a larger set of group ids at the expense of a little more memory consumption. If you do not make this configuration (i.e. use the default), then with a large number of ids (more than 1024), you will start to get hash map conflicts which can result in ActiveMQ reassigning groups to new consumers unexpectedly, which is not&nbsp;why you were using Message Groups in the first place.So that's what I've got on ActiveMQ Message Groups. I hope you find them as useful as I have.",
      tags: ["Message Groups","ActiveMQ"],
      id: 10
    });
    

    index.add({
      title: "Getting Started with FUSE ESB 4.2 Webinar",
      category: null,
      content: "I'm doing the Getting Started with FUSE ESB 4.2 Webinar tomorrow - 15-JulyAs always, I'm updating the sample code to reference the latest and greatest version of FUSE. One of the big changes I'm doing this time is making the shared WSDL file a real shared resource. Its always bugged me that I've had two+ copies of the WSDL file in the project, since it needs to be the same for everything to work correctly.I found a great article on how to do shared resources in Maven here. So we'll see how this goes…I figure (hope) its helpful to people that this project could be used as a starter template for future projects. It does make the Maven POMs more complicated than they need to be for this simple project, but it reflects the best practices that I've been able to find on the web. My hope is that if this is used as a template for more complicated projects, then all this extra effort will pay off.Hope to see you tomorrow at the webinar…",
      tags: [],
      id: 11
    });
    

    index.add({
      title: "Evaluating Open Source Integration Software",
      category: null,
      content: "I've been helping a number of companies evaluate Open Source solutions to their integration problems, and I wanted to share some thoughts (and hopefully get some feedback) on things I've seen work and not work.The biggest challenge that I've seen companies have is that they don't get the same level of help and support (for free) in evaluating Open Source versus traditional close sourced products. So the result is they underestimate the level of effort to evaluate the Open Source solution, leading to lots of frustration.Closed source companies have a product license fee that helps offset the cost of their technical field resource(s) (generally SEs) help a customer with their evaluation. This generally includes answering RFI / RFP questions, and implementing a POC scenario. SEs are used to doing many, many POCs a year, so they can provide a lot of help (though naturally biased in their product's favor) to the evaluator in best testing the integration solution and creating a presentation to their management about the results.With Open Source, there is no product license fee to help a company afford to expend the same level of free pre-sales support. This means that the evaluator is on their own to download, and implement a POC scenario to see if the Open Source solution meets their requirements. This is both good and bad. Good in that the evaluator has a really good sense at the end of the evaluation of the fit for the Open Source solution. The bad is the evaluator generally does not know how to use (or use well) the Open source product, so they have to ramp a steep learning curve quickly, and will most likely have a frustrating experience implementing the POC scenario. To get a true sense of the real fit of the Open source solution, either a great deal more time needs to be allocated to its evaluation (allow more time to ramp the learning curve), OR pay money to a knowledgeable consultant to help in your evaluation.What have your experiences been?",
      tags: [],
      id: 12
    });
    

    index.add({
      title: "Looking at JBI and Camel",
      category: null,
      content: "I've been distracted by doing a couple of webinars on Camel&nbsp;so I haven't had a lot of time to spend on digging into my original blog post&nbsp;on Camel, CXF, and ServiceMix / JBI working well together. From looking at the JBI spec it seems that for WSDL 1.x, JBI does 'mandate' using the JBI XML wrapper. My sense is that since I'm having the CXF components work with / generate from a WSDL 1.1, it seems reasonable that would wrap the XML message. The other ServiceMix components are magically generating the NMR / WSDL binding for me so apparently its up for debate if the must use the JBI wrapper element - if they generate WSDL 2.0, the wrapper element is optional.From looking at the JMS and HTTP components, its clear that there is a framework in place using Camel interceptors to add the JBI or SOAP envelopes when needed.At this point, I'm going to let the dust settle with Camel 2.0, ServiceMix 4.x, etc. before I dig in more on how to get Camel's jbi endpoint to do the right thing with that jbi message wrapper….",
      tags: ["Camel","ServiceMix"],
      id: 13
    });
    

    index.add({
      title: "Setting up Nexus as a local Maven Repository Manager",
      category: null,
      content: "I do a lot with Apache Maven, and I kept hearing about how Nexus&nbsp;helps with better managing your local Maven repository, especially if you go offline a lot, which I do. I was pleasantly surprised as how easy it was to setup and that it did actually seems to speed up my Maven project builds. Sad to get excited about a product that actually does what it claims...My needs are pretty modest in regards to Maven, and most of the Maven projects I deal with are small, so normally dealing with annoyances of offline and/or configuring m2eclipse to know about the 2 other Maven repos I use isn't that bad. However, when I started trying to build all of ServiceMix (24+ Maven projects) and started to see lots and lots of waiting on downloading various dependencies, I decided maybe I'd see if Nexus could help.The install of the Nexus Open Source is very straightforward - untaring a tar ball - and starting the Nexus server was as easy as bin/nexus start. Sonatype provides a great (free) on-line book on Nexus - Repository Management with Nexus&nbsp;- that provides good instructions on installation and initial configuration. There isn't much configuration by default except for modifying your Apache Maven ~/.m2/settings.xml file to use the Nexus server versus looking on the Internet for dependencies.The biggest challenge I had was since I was dealing with a large project which references ~12 other Maven Repositories (Nexus ships with Maven Central and Apache Repos configured) I had to add in those other repositories into Nexus so it could proxy them. Again the Nexus book provides a great section on how to know your missing a repo entry and how to add custom repos into Nexus. After I did a couple, it was very straightforward to figure out the other 10 or so custom repos that ServiceMix requires.What I saw was that now with Nexus it felt that the ServiceMix built went much faster - maybe 10-20 % - I guessing mostly from just quick all local machine lookups on dependencies. It also felt that Nexus was a little smarter about how it downloaded dependencies from the Internet, not positive on that. Regardless, I noticed a build speed up and a dramatic reduction in network traffic during the build.I also saw that using m2eclipse was much easier, which makes sense since both Nexus and m2eclipse are developed by the folks at Sonatype. m2eclipse will use you personal Maven ~/.m2/settings.xml file, which in my case is now configured to use Nexus, so I automagically get access to all those custom repositories I already configured in Nexus without having to painfully add into Eclipse. This is nice as I can now lookup archetypes in non-Central based repos, and do the \"so what dependency do I need to add for this Class\" search as well on non Maven Central hosted dependencies.Overall a positive experience. Nice work Sonatype!",
      tags: [],
      id: 14
    });
    

    index.add({
      title: "Eclipse Galileo, 64-bit Java, Java 5, and Snow Leopard...",
      category: null,
      content: "I thought I'd provide some details on the adventures I had in getting Eclipse Galileo (3.5) working on my Mac running Snow Leopard.As detailed in this great DZone article by Zviki Cohen, Snow Leopard only ships with a 64-bit version of Java 6. In fact, if you upgrade from Leopard to Snow Leopard, the installer will actually uninstall Java 5 and replace it with symbolic links to Java 6.This proved to by a challenge for my Eclipse needs as 1) you need to make a decision about which flavor (32 bit or 64 bit; Carbon or Cocoa) of Eclipse to install on the Mac, and 2) Eclipse gets confused by the symbolic link to Java 5 and thinks you really do have a real Java 5 JDK installed.After reading the Dzone article, I decided to go with the 64-bit Cocoa version as that seems in line with where Java and Mac OS are going. Also, as I was going through my installation Eclipse Galileo SR 1 (3.5 SR 1) shipped which provided 64-bit Cocoa versions for all Eclipse profiles (JEE, Java, etc.). The SR 1 release eliminated many of the hoops Zviki talks about in this article, making it a much easier path to choose. After using this version of Eclipse for about a week, I'd say I don't notice a big difference from the 32-bit version, but it certainly seems stable and having it do builds with really big projects (ServiceMix) it seems relatively fast. I do notice my Laptop fan turning on and both cores maxed out, which overall I assume is a good thing as that means that its taking full advantage of my computer during a large complex build...The next big task was fixing the Java 5 issue. This was an especial challenge / need for me since 1) most FUSE projects still support Java 5 and 2) m2eclipse (Apache Maven support in Eclipse) gets confused importing Maven projects that use Java 5 with Eclipse thinking its got a JDK 5 installed but really its Java 6 (see above note about Snow Leopard linking JDK 5 dir to JDK 6). This post at OneSwarm&nbsp;details how to hack a real JDK 5 onto your Snow Leopard OS. The highlight beingIn the terminal:Get the java 5 that was included in 10.5 \"leopard\" and unpackcd /tmp/wget http://www.cs.washington.edu/homes/isdal/snow_leopard_workaround/java.1.5.0-leopard.tar.gztar -xvzf java.1.5.0-leopard.tar.gzMove it to your System java folder (password needed)sudo mv 1.5.0 /System/Library/Frameworks/JavaVM.framework/Versions/1.5.0-leopardTell OS X that java 5 actually is java 5cd /System/Library/Frameworks/JavaVM.framework/Versions/sudo rm 1.5.0sudo ln -s 1.5.0-leopard 1.5.0I also updated the /System/Library/Frameworks/JavaVM.framework/Versions/1.5 link to point to the 1.5.0 link for completeness.After doing this I updated Eclipse to use the real JDK 5, and m2eclipse with my ServiceMix project built much better.&nbsp;Now I can do both Java 5 and 6 development on 64-bit Cocoa version of Eclipse - all nice and shiny new :-)Now onto some real coding... :-)",
      tags: ["Eclipse"],
      id: 15
    });
    

    index.add({
      title: "First Try at Blogging - Digging into ServiceMix and Camel Code",
      category: null,
      content: "Well, here's the classic \"Its my first blog post\" statement, so let's see how this goes :-)I've been working with ServiceMix and Camel since Feb 2009 when I joined the Progress FUSE technical field. Now I'm looking to take the next step and start digging much deeper into the code. So I thought this experience of digging into the code would also be a code this to start blogging about.The first thing I was going to look into is how some of the ServiceMix components handle messages with or without a JBI Message wrapper. Specifically, the CXF BC and SE components, by default, expect an XML message to be wrapped. The challenge is that messages sent from Camel via the JBI endpoint do not / can not be wrapped automatically (i.e. I'd need to code in the wrapping myself). Putting in a simple XSLT transform to wrap my XML messages in the minimal JBI message elements isn't a big deal, but its something I'd expect Camel to do for me (I am a lazy developer at heart :-) ).Right now I'm getting my development environment setup so I'm working off the ServiceMix and Camel Subversion repos. I've also taken this opportunity to setup the Nexus Open Source version to help better manage interactions with the various Maven repos, and Nexus does appear to help speed up a full ServiceMix build.Next steps are to find the JBI wrapper code in the CXF components to understand how they work. A big question for me is, it looks like the JBI Message wrapper is optional in the JBI spec, so shouldn't the servicemix-cxf components just handle the presence, or not, of the JBI wrapper automatically versus, by default, requiring other JBI components to do it? I understand that I can disable the JBI wrapper by setting useJBIWrapper=false on the CXF components, its just annoying that I have to disable something for many / most other components to interact with it.",
      tags: ["Camel"],
      id: 16
    });
    


var store = [{
    "title": "Kubernetes Ingress Past, Present, and Future",
    "link": "/ingress_and_beyond.html",
    "image": null,
    "date": "April 7, 2019",
    "category": null,
    "excerpt": "Photo by Luke Porter. Overview This post was inspired by listening to the February 19, 2019, Kubernetes Podcast, “Ingress, with..."
},{
    "title": "Automating your Services with Knative and Solo.io Gloo",
    "link": "/knative-and-gloo.html",
    "image": null,
    "date": "April 2, 2019",
    "category": null,
    "excerpt": "Knative is talked about a great deal, especially around how its capabilities can help provide more standard building blocks on..."
},{
    "title": "Kubernetes Ingress Control using Gloo",
    "link": "/kubernetes-ingress-controlling-with-gloo.html",
    "image": null,
    "date": "March 28, 2019",
    "category": null,
    "excerpt": "Kubernetes is excellent and makes it easier to create and manage highly distributed applications. A challenge then is how do..."
},{
    "title": "Canary Deployments with Gloo Function Gateway using Weighted Destinations",
    "link": "/canary-deployments-with-weighted-routes.html",
    "image": null,
    "date": "March 12, 2019",
    "category": null,
    "excerpt": "Photo by Form. This is the 3rd post in my 3 part series on doing Canary Releases with Solo.io Gloo...."
},{
    "title": "Canary Deployments with Gloo Function Gateway",
    "link": "/canary-deployments-with-solo.html",
    "image": null,
    "date": "February 19, 2019",
    "category": null,
    "excerpt": "Photo by Zab Consulting. This is the 2nd post in my 3 part series on doing Canary Releases with Solo.io..."
},{
    "title": "Routing with Gloo Function Gateway",
    "link": "/function-routing-with-gloo.html",
    "image": null,
    "date": "February 19, 2019",
    "category": null,
    "excerpt": "Photo by Pietro Jeng. This is the 1st post in my 3 part series on doing Canary Releases with Solo.io..."
},{
    "title": "New Personal Blog Site",
    "link": "/new-blog-site.html",
    "image": null,
    "date": "February 16, 2019",
    "category": null,
    "excerpt": "Photo by Sushobhan Badhai. I finally got around to updating my blog site to use Jekyll hosted on GitHub.com. I’m..."
},{
    "title": "socat is so cool...",
    "link": "/socat-is-so-cool.html",
    "image": null,
    "date": "April 22, 2011",
    "category": null,
    "excerpt": "I was helping someone the other day figure out how to use Camel to create a TCP/IP proxy, and I..."
},{
    "title": "I got an article published at JDJ...",
    "link": "/i-got-article-published-at-jdj.html",
    "image": null,
    "date": "March 23, 2011",
    "category": null,
    "excerpt": "OSGi: An Overview of Its Impact on the Software Lifecycle— OSGi technology brings a number of much needed benefits to..."
},{
    "title": "ServiceMix 4 Example Project",
    "link": "/servicemix-4-example-project.html",
    "image": null,
    "date": "November 15, 2010",
    "category": null,
    "excerpt": "I've just pushed a new version of the \"Getting Started with ServiceMix 4\" example webinar project to GitHub (https://github.com/scranton/servicemix4-example-payment-service). My..."
},{
    "title": "ActiveMQ Message Groups",
    "link": "/activemq-message-groups.html",
    "image": null,
    "date": "September 28, 2010",
    "category": null,
    "excerpt": "The JMS specification does not provide a lot of guidance about use of Message Groups. Property Name Type Set By..."
},{
    "title": "Getting Started with FUSE ESB 4.2 Webinar",
    "link": "/getting-started-with-fuse-esb-42.html",
    "image": null,
    "date": "July 14, 2010",
    "category": null,
    "excerpt": "I'm doing the Getting Started with FUSE ESB 4.2 Webinar tomorrow - 15-JulyAs always, I'm updating the sample code to..."
},{
    "title": "Evaluating Open Source Integration Software",
    "link": "/evaluating-open-source-integration.html",
    "image": null,
    "date": "April 8, 2010",
    "category": null,
    "excerpt": "I've been helping a number of companies evaluate Open Source solutions to their integration problems, and I wanted to share..."
},{
    "title": "Looking at JBI and Camel",
    "link": "/looking-at-jbi-and-camel.html",
    "image": null,
    "date": "October 22, 2009",
    "category": null,
    "excerpt": "I've been distracted by doing a couple of webinars on Camel&nbsp;so I haven't had a lot of time to spend..."
},{
    "title": "Setting up Nexus as a local Maven Repository Manager",
    "link": "/setting-up-nexus-as-local-maven.html",
    "image": null,
    "date": "October 1, 2009",
    "category": null,
    "excerpt": "I do a lot with Apache Maven, and I kept hearing about how Nexus&nbsp;helps with better managing your local Maven..."
},{
    "title": "Eclipse Galileo, 64-bit Java, Java 5, and Snow Leopard...",
    "link": "/eclipse-galileo-64-bit-java-java-5-and.html",
    "image": null,
    "date": "October 1, 2009",
    "category": null,
    "excerpt": "I thought I'd provide some details on the adventures I had in getting Eclipse Galileo (3.5) working on my Mac..."
},{
    "title": "First Try at Blogging - Digging into ServiceMix and Camel Code",
    "link": "/first-try-at-blogging-digging-into.html",
    "image": null,
    "date": "September 30, 2009",
    "category": null,
    "excerpt": "Well, here's the classic \"Its my first blog post\" statement, so let's see how this goes :-)I've been working with..."
}]

$(document).ready(function() {
    $('#search-input').on('keyup', function () {
        var resultdiv = $('#results-container');
        if (!resultdiv.is(':visible'))
            resultdiv.show();
        var query = $(this).val();
        var result = index.search(query);
        resultdiv.empty();
        $('.show-results-count').text(result.length + ' Results');
        for (var item in result) {
            var ref = result[item].ref;
            var searchitem = '<li><a href="'+ hostname + store[ref].link+'">'+store[ref].title+'</a></li>';
            resultdiv.append(searchitem);
        }
    });
});